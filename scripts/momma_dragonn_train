#!/usr/bin/env python
from __future__ import division
from __future__ import print_function
from __future__ import absolute_import
import sys
import os
import yaml
import momma_dragonn
from collections import OrderedDict
from avutils import file_processing as fp
import pdb 
def momma_dragonn_train(options):
    model_evaluator = momma_dragonn.loaders.load_model_evaluator(
                            options.evaluator_config)
    larger_is_better = model_evaluator.is_larger_better_for_key_metric()

    end_of_epoch_callbacks = momma_dragonn.loaders.load_end_of_epoch_callbacks(
                                config=options.end_of_epoch_callbacks_config)

    end_of_training_callbacks =\
        momma_dragonn.loaders.load_end_of_training_callbacks(
            config=options.end_of_training_callbacks_config,
            key_metric_name=model_evaluator.get_key_metric_name(),
            larger_is_better=larger_is_better)

    list_of_hyperparameter_settings =\
        momma_dragonn.loaders.load_hyperparameter_configs_list(
                                        options.hyperparameter_configs_list)
    for hyperparameter_setting in list_of_hyperparameter_settings:
        data_loaders =  hyperparameter_setting["data_loaders"]
        print("got data loaders!")
        model_creator = hyperparameter_setting["model_creator"]
        print("got model creator") 
        model_trainer = hyperparameter_setting["model_trainer"]
        print("got model trainer!") 
        message = hyperparameter_setting["message"]
        model_wrapper, performance_history, training_metadata =\
            model_trainer.train(
                model_wrapper=model_creator.get_model_wrapper(),
                model_evaluator=model_evaluator,
                data_loaders=data_loaders,
                end_of_epoch_callbacks=end_of_epoch_callbacks,
                error_callbacks=[])

        model_trainer_config = model_trainer

        #if trained for at least one epoch:
        if (performance_history.get_best_valid_epoch_perf_info is not None):
            for end_of_training_callback in end_of_training_callbacks:
                end_of_training_callback( #handles writing to db
    performance_history=performance_history,
    model_wrapper=model_wrapper,
    training_metadata=training_metadata,
    message=message,
    model_creator_info=model_creator.get_jsonable_object(),
    model_trainer_info=model_trainer.get_jsonable_object(),
    data_loaders_info=\
        OrderedDict([(split_name, data_loader.get_jsonable_object())
        for (split_name, data_loader) in data_loaders.items()]))

if __name__ == "__main__":
    import argparse;
    parser = argparse.ArgumentParser()
    parser.add_argument("--evaluator_config")
    parser.add_argument("--end_of_epoch_callbacks_config")
    parser.add_argument("--end_of_training_callbacks_config")
    parser.add_argument("--hyperparameter_configs_list")
    options = parser.parse_args()
    momma_dragonn_train(options)
