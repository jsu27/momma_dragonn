{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import simdna\n",
    "import simdna.synthetic as synthetic\n",
    "import avutils\n",
    "from avutils import util\n",
    "import numpy as np\n",
    "import momma_dragonn\n",
    "reload(momma_dragonn)\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_sequences_set(seq_length, num_seqs, motif_names, mean_motifs=1, min_motifs=0, max_motifs=2, zero_prob=0):\n",
    "    loadedMotifs = synthetic.LoadedEncodeMotifs(simdna.ENCODE_MOTIFS_PATH, pseudocountProb=0.001)\n",
    "    embedInBackground = synthetic.EmbedInABackground(\n",
    "        backgroundGenerator=synthetic.ZeroOrderBackgroundGenerator(seqLength=seq_length)\n",
    "        , embedders=[\n",
    "            synthetic.RepeatedEmbedder(\n",
    "            synthetic.SubstringEmbedder(\n",
    "                #synthetic.ReverseComplementWrapper(\n",
    "                substringGenerator=synthetic.PwmSamplerFromLoadedMotifs(\n",
    "                    loadedMotifs=loadedMotifs,motifName=motifName)\n",
    "                #),\n",
    "                ,positionGenerator=synthetic.UniformPositionGenerator()),\n",
    "            quantityGenerator=synthetic.ZeroInflater(synthetic.MinMaxWrapper(\n",
    "                synthetic.PoissonQuantityGenerator(mean_motifs),\n",
    "                theMax=max_motifs, theMin=min_motifs), zeroProb=zero_prob)\n",
    "            )\n",
    "            for motifName in motif_names\n",
    "        ]\n",
    "    )\n",
    "    sequenceSetGenerator = synthetic.GenerateSequenceNTimes(embedInBackground, num_seqs)\n",
    "    return sequenceSetGenerator\n",
    "\n",
    "def one_hot_encode_sequences_set(sequence_set_generator):\n",
    "    one_hot_encoded_sequences = []\n",
    "    for sequence in sequence_set_generator.generateSequences():\n",
    "        one_hot_encoded_sequences.append(avutils.util.seq_to_2d_image(sequence.seq))\n",
    "    return np.array(one_hot_encoded_sequences)\n",
    "\n",
    "seq_length=1000\n",
    "\n",
    "\n",
    "motif_names = [\"CTCF_known1\", \"IRF_known1\", \"GATA_disc1\"]\n",
    "\n",
    "one_hot_data_train = one_hot_encode_sequences_set(\n",
    "                generate_sequences_set(\n",
    "                    seq_length=seq_length, num_seqs=5000, motif_names=motif_names))\n",
    "\n",
    "one_hot_data_valid = one_hot_encode_sequences_set(\n",
    "                        generate_sequences_set(\n",
    "                        seq_length=seq_length, num_seqs=1000, motif_names=motif_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function set_subtensor at 0x1165989b0>\n",
      "('Input modes', ['sequence'])\n",
      "('Output modes', ['output'])\n",
      "Loading momma_dragonn.stopping_criteria.EarlyStopping\n",
      "Loading validation data into memory\n",
      "Loaded\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.3537 - val_loss: 1.3185\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.3087 - val_loss: 1.3050\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.3038 - val_loss: 1.3035\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.3021 - val_loss: 1.3013\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2995 - val_loss: 1.2992\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2973 - val_loss: 1.2971\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2957 - val_loss: 1.2962\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2951 - val_loss: 1.2957\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2947 - val_loss: 1.2953\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2943 - val_loss: 1.2951\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2940 - val_loss: 1.2949\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2938 - val_loss: 1.2946\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2936 - val_loss: 1.2945\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2932 - val_loss: 1.2943\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2932 - val_loss: 1.2942\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2929 - val_loss: 1.2940\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2928 - val_loss: 1.2939\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2927 - val_loss: 1.2938\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2925 - val_loss: 1.2937\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2924 - val_loss: 1.2936\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2922 - val_loss: 1.2935\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2922 - val_loss: 1.2935\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2921 - val_loss: 1.2933\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2918 - val_loss: 1.2933\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2918 - val_loss: 1.2932\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2917 - val_loss: 1.2931\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2914 - val_loss: 1.2929\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2913 - val_loss: 1.2928\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2912 - val_loss: 1.2926\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2909 - val_loss: 1.2924\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2908 - val_loss: 1.2922\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2904 - val_loss: 1.2918\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2901 - val_loss: 1.2914\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2899 - val_loss: 1.2910\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2893 - val_loss: 1.2907\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2892 - val_loss: 1.2905\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2890 - val_loss: 1.2903\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2888 - val_loss: 1.2901\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2885 - val_loss: 1.2900\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2885 - val_loss: 1.2898\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2882 - val_loss: 1.2896\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2881 - val_loss: 1.2895\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2880 - val_loss: 1.2893\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2878 - val_loss: 1.2892\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2877 - val_loss: 1.2891\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2875 - val_loss: 1.2889\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2875 - val_loss: 1.2888\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2874 - val_loss: 1.2887\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2871 - val_loss: 1.2885\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2871 - val_loss: 1.2884\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2869 - val_loss: 1.2883\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2868 - val_loss: 1.2882\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2866 - val_loss: 1.2881\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2867 - val_loss: 1.2880\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2864 - val_loss: 1.2879\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2864 - val_loss: 1.2878\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2862 - val_loss: 1.2877\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2862 - val_loss: 1.2875\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2862 - val_loss: 1.2874\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2859 - val_loss: 1.2874\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2860 - val_loss: 1.2872\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2859 - val_loss: 1.2872\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2858 - val_loss: 1.2871\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2856 - val_loss: 1.2870\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2857 - val_loss: 1.2869\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2855 - val_loss: 1.2869\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2854 - val_loss: 1.2868\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2854 - val_loss: 1.2868\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2853 - val_loss: 1.2867\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2853 - val_loss: 1.2866\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2852 - val_loss: 1.2866\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2853 - val_loss: 1.2866\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2852 - val_loss: 1.2865\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2850 - val_loss: 1.2865\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2852 - val_loss: 1.2865\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2850 - val_loss: 1.2864\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2850 - val_loss: 1.2864\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2849 - val_loss: 1.2864\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2850 - val_loss: 1.2864\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2849 - val_loss: 1.2863\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2849 - val_loss: 1.2863\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2848 - val_loss: 1.2863\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2849 - val_loss: 1.2862\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2849 - val_loss: 1.2863\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2847 - val_loss: 1.2862\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2849 - val_loss: 1.2862\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2848 - val_loss: 1.2862\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2848 - val_loss: 1.2861\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2847 - val_loss: 1.2861\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2847 - val_loss: 1.2861\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2847 - val_loss: 1.2861\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2846 - val_loss: 1.2861\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2846 - val_loss: 1.2860\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2846 - val_loss: 1.2860\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2846 - val_loss: 1.2860\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2845 - val_loss: 1.2860\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2846 - val_loss: 1.2859\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2845 - val_loss: 1.2859\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2844 - val_loss: 1.2859\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2845 - val_loss: 1.2859\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2844 - val_loss: 1.2858\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2844 - val_loss: 1.2858\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2844 - val_loss: 1.2858\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2845 - val_loss: 1.2858\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2844 - val_loss: 1.2858\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 18s - loss: 1.2843 - val_loss: 1.2858\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2842 - val_loss: 1.2858\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2843 - val_loss: 1.2857\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2844 - val_loss: 1.2857\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2842 - val_loss: 1.2857\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2844 - val_loss: 1.2857\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2843 - val_loss: 1.2856\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2842 - val_loss: 1.2856\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2842 - val_loss: 1.2856\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2842 - val_loss: 1.2855\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2841 - val_loss: 1.2855\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2841 - val_loss: 1.2855\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2841 - val_loss: 1.2855\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2840 - val_loss: 1.2855\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2841 - val_loss: 1.2855\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2840 - val_loss: 1.2855\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2841 - val_loss: 1.2854\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2840 - val_loss: 1.2854\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2839 - val_loss: 1.2854\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2840 - val_loss: 1.2854\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2839 - val_loss: 1.2853\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2839 - val_loss: 1.2853\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2838 - val_loss: 1.2853\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2839 - val_loss: 1.2853\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2838 - val_loss: 1.2853\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2838 - val_loss: 1.2853\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2837 - val_loss: 1.2852\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2838 - val_loss: 1.2852\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2839 - val_loss: 1.2852\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2837 - val_loss: 1.2851\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2838 - val_loss: 1.2851\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2838 - val_loss: 1.2851\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2837 - val_loss: 1.2851\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2836 - val_loss: 1.2851\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2837 - val_loss: 1.2851\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2836 - val_loss: 1.2850\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2836 - val_loss: 1.2850\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2836 - val_loss: 1.2850\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2835 - val_loss: 1.2850\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2836 - val_loss: 1.2850\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2835 - val_loss: 1.2850\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2836 - val_loss: 1.2849\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2836 - val_loss: 1.2849\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2834 - val_loss: 1.2849\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2836 - val_loss: 1.2849\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2835 - val_loss: 1.2849\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2834 - val_loss: 1.2849\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2834 - val_loss: 1.2848\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2835 - val_loss: 1.2848\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2834 - val_loss: 1.2848\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2834 - val_loss: 1.2848\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2833 - val_loss: 1.2848\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2834 - val_loss: 1.2848\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2834 - val_loss: 1.2847\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2832 - val_loss: 1.2847\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2834 - val_loss: 1.2847\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2834 - val_loss: 1.2847\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2833 - val_loss: 1.2847\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2832 - val_loss: 1.2847\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2833 - val_loss: 1.2847\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2832 - val_loss: 1.2847\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2832 - val_loss: 1.2846\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2832 - val_loss: 1.2846\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2832 - val_loss: 1.2846\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2832 - val_loss: 1.2846\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2831 - val_loss: 1.2846\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2833 - val_loss: 1.2845\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2832 - val_loss: 1.2845\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2831 - val_loss: 1.2845\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2832 - val_loss: 1.2845\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2832 - val_loss: 1.2845\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2831 - val_loss: 1.2845\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2830 - val_loss: 1.2845\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2832 - val_loss: 1.2845\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2830 - val_loss: 1.2844\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2831 - val_loss: 1.2844\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2830 - val_loss: 1.2844\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2830 - val_loss: 1.2844\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2831 - val_loss: 1.2844\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2829 - val_loss: 1.2844\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2831 - val_loss: 1.2844\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2831 - val_loss: 1.2844\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2830 - val_loss: 1.2843\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2829 - val_loss: 1.2843\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2830 - val_loss: 1.2843\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2829 - val_loss: 1.2843\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2829 - val_loss: 1.2843\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2830 - val_loss: 1.2843\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2829 - val_loss: 1.2843\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2830 - val_loss: 1.2843\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2829 - val_loss: 1.2843\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2830 - val_loss: 1.2843\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2830 - val_loss: 1.2843\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2828 - val_loss: 1.2843\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2829 - val_loss: 1.2843\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2829 - val_loss: 1.2843\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2828 - val_loss: 1.2842\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2828 - val_loss: 1.2842\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2830 - val_loss: 1.2842\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2828 - val_loss: 1.2842\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2829 - val_loss: 1.2842\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2828 - val_loss: 1.2842\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2828 - val_loss: 1.2842\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2829 - val_loss: 1.2842\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2827 - val_loss: 1.2842\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2829 - val_loss: 1.2842\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2829 - val_loss: 1.2842\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2828 - val_loss: 1.2842\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2827 - val_loss: 1.2841\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2828 - val_loss: 1.2842\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2827 - val_loss: 1.2841\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2828 - val_loss: 1.2841\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2828 - val_loss: 1.2841\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2827 - val_loss: 1.2841\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2828 - val_loss: 1.2841\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2827 - val_loss: 1.2841\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2828 - val_loss: 1.2841\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2828 - val_loss: 1.2841\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2827 - val_loss: 1.2841\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2828 - val_loss: 1.2841\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2828 - val_loss: 1.2841\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2827 - val_loss: 1.2841\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2827 - val_loss: 1.2841\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2828 - val_loss: 1.2841\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2827 - val_loss: 1.2840\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2827 - val_loss: 1.2840\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2826 - val_loss: 1.2840\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2827 - val_loss: 1.2840\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2828 - val_loss: 1.2840\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2826 - val_loss: 1.2840\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2828 - val_loss: 1.2840\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2828 - val_loss: 1.2840\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2827 - val_loss: 1.2840\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2826 - val_loss: 1.2840\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2827 - val_loss: 1.2840\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2826 - val_loss: 1.2839\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2826 - val_loss: 1.2839\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2827 - val_loss: 1.2839\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2826 - val_loss: 1.2839\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2827 - val_loss: 1.2839\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2826 - val_loss: 1.2839\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2827 - val_loss: 1.2839\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2827 - val_loss: 1.2839\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 16s - loss: 1.2826 - val_loss: 1.2839\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2827 - val_loss: 1.2839\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2827 - val_loss: 1.2839\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2826 - val_loss: 1.2839\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2825 - val_loss: 1.2839\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2827 - val_loss: 1.2839\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2826 - val_loss: 1.2839\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2826 - val_loss: 1.2839\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2825 - val_loss: 1.2839\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2826 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2827 - val_loss: 1.2839\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2825 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2827 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2827 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2826 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2825 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2826 - val_loss: 1.2839\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2825 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2825 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2826 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2825 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2826 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2825 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2826 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2826 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2825 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2826 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2826 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2825 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2825 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2826 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2825 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2826 - val_loss: 1.2838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 17s - loss: 1.2825 - val_loss: 1.2838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "reload(keras)\n",
    "reload(keras.objectives)\n",
    "from keras import backend\n",
    "reload(backend)\n",
    "import keras.layers.convolutional\n",
    "reload(keras.layers.convolutional)\n",
    "import momma_dragonn\n",
    "reload(momma_dragonn)\n",
    "reload(momma_dragonn.end_of_epoch_callbacks)\n",
    "reload(momma_dragonn.data_loaders)\n",
    "reload(momma_dragonn.model_evaluators)\n",
    "reload(momma_dragonn.model_trainers.keras_model_trainer)\n",
    "reload(momma_dragonn.data_loaders.core)\n",
    "\n",
    "def model_creator_func():\n",
    "    filter_width=20\n",
    "    maxpool_filter_width=seq_length-(filter_width-1) #pool over entire region for now\n",
    "    \n",
    "    from keras.models import Graph\n",
    "    graph = Graph() \n",
    "    graph.add_input(name=\"sequence\", input_shape=(1,4,seq_length))\n",
    "    #add convolutional layer\n",
    "    graph.add_node(\n",
    "        keras.layers.convolutional.Convolution2D(nb_filter=5*len(motif_names), nb_row=4, nb_col=filter_width),\n",
    "        name=\"conv\", input=\"sequence\")\n",
    "    #add maxpool filter layer\n",
    "    graph.add_node(\n",
    "        keras.layers.convolutional.MaxPoolFilter2D(pool_size=(1,maxpool_filter_width)),\n",
    "        name=\"filt\", input=\"conv\")\n",
    "    #add a padding layer so deconv will be the right size\n",
    "    graph.add_node(\n",
    "        keras.layers.convolutional.ZeroPadding2D(padding=(0,filter_width-1)),\n",
    "        name=\"padding\", input=\"filt\")\n",
    "    #add a deconv layer to reconstruct the input\n",
    "    graph.add_node(\n",
    "        keras.layers.convolutional.Convolution2D(\n",
    "            nb_filter=4, nb_row=1, nb_col=filter_width,\n",
    "            W_constraint=keras.constraints.MaxNorm(m=10)),\n",
    "        name=\"deconv\", input=\"padding\")\n",
    "    #transpose to make the deconv axis the row axis\n",
    "    graph.add_node(\n",
    "        keras.layers.convolutional.ExchangeChannelsAndRows(),\n",
    "        name=\"swapaxes\", input=\"deconv\")\n",
    "    #softmax across rows\n",
    "    graph.add_node(\n",
    "        keras.layers.convolutional.SoftmaxAcrossRows(),\n",
    "        name=\"output_softmax\", input=\"swapaxes\")\n",
    "    #designate output node\n",
    "    graph.add_output(name=\"output\", input=\"output_softmax\")\n",
    "    #compile\n",
    "    graph.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss={\"output\": \"one_hot_rows_categorical_cross_entropy\"}\n",
    "    )\n",
    "    return graph\n",
    "\n",
    "#model creator\n",
    "model_creator = momma_dragonn.model_creators.flexible_keras.KerasModelFromFunc(\n",
    "    func=model_creator_func,\n",
    "    model_wrapper_class=momma_dragonn.model_wrappers.keras_model_wrappers.KerasGraphModelWrapper)    \n",
    "\n",
    "#data loaders\n",
    "train_data_loader = momma_dragonn.data_loaders.core.BatchDataLoader_XYDictAPI(\n",
    "                        X={'sequence': one_hot_data_train}, Y={'output': one_hot_data_train},\n",
    "                        weight={}, batch_size=20, num_to_load_for_eval=1000, bundle_x_and_y_in_generator=True)\n",
    "valid_data_loader = momma_dragonn.data_loaders.core.AtOnceDataLoader_XYDictAPI(\n",
    "                        X={'sequence': one_hot_data_valid}, Y={'output': one_hot_data_valid})\n",
    "#model evaluator\n",
    "model_evaluator = momma_dragonn.model_evaluators.GraphAccuracyStats(\n",
    "    key_metric=\"onehot_rows_crossent\", all_metrics=[\"onehot_rows_crossent\"])\n",
    "\n",
    "#stopping criterion\n",
    "stopping_criterion_config = {\"class\": \"EarlyStopping\", \"kwargs\": {\"max_epochs\": 300, \"epochs_to_wait\": 10}}\n",
    "\n",
    "#callbacks\n",
    "end_of_epoch_callbacks = []#momma_dragonn.end_of_epoch_callbacks.PrintPerfAfterEpoch(print_trend=True)]\n",
    "\n",
    "#trainer\n",
    "trainer = momma_dragonn.model_trainers.keras_model_trainer.KerasFitGeneratorModelTrainer(\n",
    "    samples_per_epoch=3000, stopping_criterion_config=stopping_criterion_config)\n",
    "\n",
    "#train model\n",
    "model_wrapper, performance_history, training_metadata = trainer.train(model_wrapper=model_creator.get_model_wrapper(),\n",
    "                                                          model_evaluator=model_evaluator,\n",
    "                                                          valid_data_loader=valid_data_loader,\n",
    "                                                          other_data_loaders={'train': train_data_loader},\n",
    "                                                          end_of_epoch_callbacks=end_of_epoch_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_wrapper.create_files_to_save(directory=\".\", prefix=\"first_autoencoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_yaml                                    \n",
    "model = model_from_yaml(open(\"first_autoencode_attempt_modelYaml.yaml\").read())\n",
    "model.load_weights(\"first_autoencode_attempt_modelWeights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deconv_weights, deconv_biases = model.nodes['deconv'].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[             nan,              nan,              nan, ...,\n",
       "                       nan,              nan,              nan]],\n",
       "\n",
       "        [[             nan,              nan,              nan, ...,\n",
       "                       nan,              nan,              nan]],\n",
       "\n",
       "        [[             nan,              nan,              nan, ...,\n",
       "                       nan,              nan,              nan]],\n",
       "\n",
       "        ..., \n",
       "        [[             nan,              nan,              nan, ...,\n",
       "                       nan,              nan,              nan]],\n",
       "\n",
       "        [[             nan,              nan,              nan, ...,\n",
       "                       nan,              nan,              nan]],\n",
       "\n",
       "        [[ -4.25352933e+37,              nan,  -3.22772963e+19, ...,\n",
       "                       nan,              nan,  -2.65845583e+36]]],\n",
       "\n",
       "\n",
       "       [[[             nan,              nan,              nan, ...,\n",
       "           -3.68214284e+19,              nan,              nan]],\n",
       "\n",
       "        [[             nan,              nan,              nan, ...,\n",
       "                       nan,              nan,              nan]],\n",
       "\n",
       "        [[             nan,              nan,              nan, ...,\n",
       "                       nan,              nan,  -1.69808866e+38]],\n",
       "\n",
       "        ..., \n",
       "        [[             nan,              nan,              nan, ...,\n",
       "                       nan,              nan,              nan]],\n",
       "\n",
       "        [[             nan,              nan,              nan, ...,\n",
       "                       nan,              nan,   1.03845931e+34]],\n",
       "\n",
       "        [[             nan,              nan,  -3.68934859e+19, ...,\n",
       "                       nan,              nan,              nan]]],\n",
       "\n",
       "\n",
       "       [[[             nan,              nan,              nan, ...,\n",
       "           -3.37623891e+38,              nan,  -3.40282326e+38]],\n",
       "\n",
       "        [[             nan,              nan,              nan, ...,\n",
       "                       nan,              nan,              nan]],\n",
       "\n",
       "        [[             nan,              nan,              nan, ...,\n",
       "                       nan,              nan,              nan]],\n",
       "\n",
       "        ..., \n",
       "        [[             nan,              nan,  -4.25352933e+37, ...,\n",
       "                       nan,              nan,  -3.40280967e+38]],\n",
       "\n",
       "        [[             nan,              nan,              nan, ...,\n",
       "                       nan,              nan,              nan]],\n",
       "\n",
       "        [[             nan,              nan,              nan, ...,\n",
       "                       nan,              nan,              nan]]],\n",
       "\n",
       "\n",
       "       [[[             nan,  -7.62939226e-06,  -4.12060653e+37, ...,\n",
       "           -1.99999988e+00,              nan,              nan]],\n",
       "\n",
       "        [[             nan,              nan,              nan, ...,\n",
       "                       nan,  -4.25352933e+37,              nan]],\n",
       "\n",
       "        [[             nan,              nan,              nan, ...,\n",
       "                       nan,              nan,              nan]],\n",
       "\n",
       "        ..., \n",
       "        [[             nan,              nan,              nan, ...,\n",
       "                       nan,              nan,              nan]],\n",
       "\n",
       "        [[ -1.03845931e+34,              nan,              nan, ...,\n",
       "                       nan,              nan,              nan]],\n",
       "\n",
       "        [[  3.68934859e+19,              nan,              nan, ...,\n",
       "                       nan,  -2.55211755e+38,              nan]]]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deconv_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
